{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m7rU-pjX3Y1O"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install gradio transformers accelerate numpy\n",
        "!pip install torch torchvision av hf_xet spaces\n",
        "!pip install pillow huggingface_hub opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZUVag_jJMck"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login, HfApi\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW4MjaOs3c9E"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from transformers import AutoProcessor, TextIteratorStreamer, AutoModelForImageTextToText\n",
        "from transformers.image_utils import load_image\n",
        "from threading import Thread\n",
        "import time\n",
        "import torch\n",
        "import spaces\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Helper: progress bar HTML\n",
        "def progress_bar_html(label: str) -> str:\n",
        "    return f'''\n",
        "<div style=\"display: flex; align-items: center;\">\n",
        "    <span style=\"margin-right: 10px; font-size: 14px;\">{label}</span>\n",
        "    <div style=\"width: 110px; height: 5px; background-color: #FFB6C1; border-radius: 2px; overflow: hidden;\">\n",
        "        <div style=\"width: 100%; height: 100%; background-color: #FF69B4; animation: loading 1.5s linear infinite;\"></div>\n",
        "    </div>\n",
        "</div>\n",
        "<style>\n",
        "@keyframes loading {{\n",
        "    0% {{ transform: translateX(-100%); }}\n",
        "    100% {{ transform: translateX(100%); }}\n",
        "}}\n",
        "</style>\n",
        "    '''\n",
        "\n",
        "# Aya Vision 8B setup\n",
        "AYA_MODEL_ID = \"CohereForAI/aya-vision-8b\"\n",
        "aya_processor = AutoProcessor.from_pretrained(AYA_MODEL_ID)\n",
        "aya_model = AutoModelForImageTextToText.from_pretrained(\n",
        "    AYA_MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "def downsample_video(video_path, num_frames=10):\n",
        "    \"\"\"\n",
        "    Extract evenly spaced frames and timestamps from a video file.\n",
        "    Returns list of (PIL.Image, timestamp_sec).\n",
        "    \"\"\"\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    total = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    fps = vidcap.get(cv2.CAP_PROP_FPS) or 30\n",
        "    indices = np.linspace(0, total-1, num_frames, dtype=int)\n",
        "    frames = []\n",
        "    for idx in indices:\n",
        "        vidcap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
        "        ret, frame = vidcap.read()\n",
        "        if not ret:\n",
        "            continue\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        pil = Image.fromarray(frame)\n",
        "        timestamp = round(idx / fps, 2)\n",
        "        frames.append((pil, timestamp))\n",
        "    vidcap.release()\n",
        "    return frames\n",
        "\n",
        "@spaces.GPU\n",
        "def process_image(prompt: str, image: Image.Image):\n",
        "    if image is None:\n",
        "        yield \"Error: Please upload an image.\"\n",
        "        return\n",
        "    if not prompt.strip():\n",
        "        yield \"Error: Please provide a prompt with the image.\"\n",
        "        return\n",
        "    yield progress_bar_html(\"Processing Image with Aya Vision 8B\")\n",
        "    messages = [{\"role\": \"user\", \"content\": [\n",
        "        {\"type\": \"image\", \"image\": image},\n",
        "        {\"type\": \"text\", \"text\": prompt.strip()}\n",
        "    ]}]\n",
        "    inputs = aya_processor.apply_chat_template(\n",
        "        messages, padding=True, add_generation_prompt=True,\n",
        "        tokenize=True, return_dict=True, return_tensors=\"pt\"\n",
        "    ).to(aya_model.device)\n",
        "    streamer = TextIteratorStreamer(aya_processor, skip_prompt=True, skip_special_tokens=True)\n",
        "    thread = Thread(target=aya_model.generate, kwargs={**inputs, \"streamer\": streamer, \"max_new_tokens\": 1024, \"do_sample\": True, \"temperature\": 0.3})\n",
        "    thread.start()\n",
        "    buff = \"\"\n",
        "    for chunk in streamer:\n",
        "        buff += chunk.replace(\"<|im_end|>\", \"\")\n",
        "        time.sleep(0.01)\n",
        "        yield buff\n",
        "\n",
        "@spaces.GPU\n",
        "def process_video(prompt: str, video_file: str):\n",
        "    if video_file is None:\n",
        "        yield \"Error: Please upload a video.\"\n",
        "        return\n",
        "    if not prompt.strip():\n",
        "        yield \"Error: Please provide a prompt with the video.\"\n",
        "        return\n",
        "    yield progress_bar_html(\"Processing Video with Aya Vision 8B\")\n",
        "    frames = downsample_video(video_file)\n",
        "    # Build chat messages with each frame and timestamp\n",
        "    content = [{\"type\": \"text\", \"text\": prompt.strip()}]\n",
        "    for img, ts in frames:\n",
        "        content.append({\"type\": \"text\", \"text\": f\"Frame at {ts}s:\"})\n",
        "        content.append({\"type\": \"image\", \"image\": img})\n",
        "    messages = [{\"role\": \"user\", \"content\": content}]\n",
        "    inputs = aya_processor.apply_chat_template(\n",
        "        messages, tokenize=True, add_generation_prompt=True,\n",
        "        return_dict=True, return_tensors=\"pt\"\n",
        "    ).to(aya_model.device)\n",
        "    streamer = TextIteratorStreamer(aya_processor, skip_prompt=True, skip_special_tokens=True)\n",
        "    thread = Thread(target=aya_model.generate, kwargs={**inputs, \"streamer\": streamer, \"max_new_tokens\": 1024, \"do_sample\": True, \"temperature\": 0.3})\n",
        "    thread.start()\n",
        "    buff = \"\"\n",
        "    for chunk in streamer:\n",
        "        buff += chunk.replace(\"<|im_end|>\", \"\")\n",
        "        time.sleep(0.01)\n",
        "        yield buff\n",
        "\n",
        "# Build Gradio UI\n",
        "demo = gr.Blocks()\n",
        "with demo:\n",
        "    gr.Markdown(\"# **Aya Vision 8B Multimodal: Image & Video**\")\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"Image Inference\"):\n",
        "            txt_i = gr.Textbox(label=\"Prompt\", placeholder=\"Enter prompt...\")\n",
        "            img_u = gr.Image(type=\"filepath\", label=\"Image\")\n",
        "            btn_i = gr.Button(\"Run Image\")\n",
        "            out_i = gr.Textbox(label=\"Output\", interactive=False)\n",
        "            btn_i.click(fn=process_image, inputs=[txt_i, img_u], outputs=out_i)\n",
        "        with gr.TabItem(\"Video Inference\"):\n",
        "            txt_v = gr.Textbox(label=\"Prompt\", placeholder=\"Enter prompt...\")\n",
        "            vid_u = gr.Video(label=\"Video\")\n",
        "            btn_v = gr.Button(\"Run Video\")\n",
        "            out_v = gr.Textbox(label=\"Output\", interactive=False)\n",
        "            btn_v.click(fn=process_video, inputs=[txt_v, vid_u], outputs=out_v)\n",
        "\n",
        "demo.launch(debug=True, share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
